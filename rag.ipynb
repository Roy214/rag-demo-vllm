{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Efficient Memory Management for Large Language\\nModel Serving with PagedAttention\\nWoosuk Kwon1,âˆ— Zhuohan Li1,âˆ— Siyuan Zhuang1 Ying Sheng1,2 Lianmin Zheng1 Cody Hao Yu3\\nJoseph E. Gonzalez1 Hao Zhang4 Ion Stoica1\\n1UC Berkeley 2Stanford University 3Independent Researcher 4UC San Diego\\nAbstract\\nHigh throughput serving of large language models (LLMs)\\nrequires batching sufficiently many requests at a time. How-\\never, existing systems struggle because the key-value cache\\n(KV cache) memory for each request is huge and grows\\nand shrinks dynamically. When managed inefficiently, this\\nmemory can be significantly wasted by fragmentation and\\nredundant duplication, limiting the batch size. To address\\nthis problem, we propose PagedAttention, an attention al-\\ngorithm inspired by the classical virtual memory and pag-\\ning techniques in operating systems. On top of it, we build\\nvLLM, an LLM serving system that achieves (1) near-zero\\nwaste in KV cache memory and (2) flexible sharing of KV\\ncache within and across requests to further reduce mem-\\nory usage. Our evaluations show that vLLM improves the\\nthroughput of popular LLMs by 2-4 Ã—with the same level\\nof latency compared to the state-of-the-art systems, such\\nas FasterTransformer and Orca. The improvement is more\\npronounced with longer sequences, larger models, and more\\ncomplex decoding algorithms. vLLMâ€™s source code is publicly\\navailable at https://github.com/vllm-project/vllm.\\n1 Introduction\\nThe emergence of large language models (LLMs) like GPT [5,\\n37] and PaLM [9] have enabled new applications such as pro-\\ngramming assistants [6, 18] and universal chatbots [19, 35]\\nthat are starting to profoundly impact our work and daily\\nroutines. Many cloud companies [34, 44] are racing to pro-\\nvide these applications as hosted services. However, running\\nthese applications is very expensive, requiring a large num-\\nber of hardware accelerators such as GPUs. According to\\nrecent estimates, processing an LLM request can be 10Ã—more\\nexpensive than a traditional keyword query [43]. Given these\\nhigh costs, increasing the throughputâ€”and hence reducing\\nPermission to make digital or hard copies of part or all of this work for\\npersonal or classroom use is granted without fee provided that copies are\\nnot made or distributed for profit or commercial advantage and that copies\\nbear this notice and the full citation on the first page. Copyrights for third-\\nparty components of this work must be honored. For all other uses, contact\\nthe owner/author(s).\\nSOSP â€™23, October 23â€“26, 2023, Koblenz, Germany\\nÂ© 2023 Copyright held by the owner/author(s).\\nACM ISBN 979-8-4007-0229-7/23/10.\\nhttps://doi.org/10.1145/3600006.3613165\\nNVIDIA A100 40GB\\nParameters \\n(26GB, 65%)\\nKV \\nCache\\n(>30%)\\nOthers\\n20\\n30\\n40Memory usage (GB)\\nParameter size\\nExisting systems vLLM\\n0 10 20 30 40\\nBatch size (# requests)\\n0\\n0.4k\\n0.8k\\n1.2kThroughput (token/s)\\nFigure 1. Left: Memory layout when serving an LLM with\\n13B parameters on NVIDIA A100. The parameters (gray)\\npersist in GPU memory throughout serving. The memory\\nfor the KV cache (red) is (de)allocated per serving request.\\nA small amount of memory (yellow) is used ephemerally\\nfor activation. Right: vLLM smooths out the rapid growth\\ncurve of KV cache memory seen in existing systems [31, 60],\\nleading to a notable boost in serving throughput.\\nthe cost per requestâ€”of LLM serving systems is becoming\\nmore important.\\nAt the core of LLMs lies an autoregressive Transformer\\nmodel [53]. This model generates words (tokens), one at a\\ntime, based on the input (prompt) and the previous sequence\\nof the outputâ€™s tokens it has generated so far. For each re-\\nquest, this expensive process is repeated until the model out-\\nputs a termination token. This sequential generation process\\nmakes the workload memory-bound, underutilizing the com-\\nputation power of GPUs and limiting the serving throughput.\\nImproving the throughput is possible by batching multi-\\nple requests together. However, to process many requests\\nin a batch, the memory space for each request should be\\nefficiently managed. For example, Fig. 1 (left) illustrates the\\nmemory distribution for a 13B-parameter LLM on an NVIDIA\\nA100 GPU with 40GB RAM. Approximately 65% of the mem-\\nory is allocated for the model weights, which remain static\\nduring serving. Close to 30% of the memory is used to store\\nthe dynamic states of the requests. For Transformers, these\\nstates consist of the key and value tensors associated with the\\nattention mechanism, commonly referred to asKV cache [41],\\nwhich represent the context from earlier tokens to gener-\\nate new output tokens in sequence. The remaining small\\nâˆ—Equal contribution.\\n1\\narXiv:2309.06180v1  [cs.LG]  12 Sep 2023', metadata={'source': 'vllm.pdf', 'page': 0}),\n",
       " Document(page_content='Orca\\n(Max)\\nOrca\\n(Pow2)\\nOrca\\n(Oracle)\\nvLLM\\n0\\n20\\n40\\n60\\n80\\n100KV cache usage (%)\\n20.4\\n13.3\\n57.3\\n8.9\\n26.8\\n17.9\\n13.6\\n41.6\\n38.2\\n25.2\\n36.6\\n96.3\\nT oken states Reservation Internal frag. External frag.\\n& Others\\nFigure 2. Average percentage of memory wastes in different\\nLLM serving systems during the experiment in Â§6.2.\\npercentage of memory is used for other data, including ac-\\ntivations â€“ the ephemeral tensors created when evaluating\\nthe LLM. Since the model weights are constant and the ac-\\ntivations only occupy a small fraction of the GPU memory,\\nthe way the KV cache is managed is critical in determining\\nthe maximum batch size. When managed inefficiently, the\\nKV cache memory can significantly limit the batch size and\\nconsequently the throughput of the LLM, as illustrated in\\nFig. 1 (right).\\nIn this paper, we observe that existing LLM serving sys-\\ntems [31, 60] fall short of managing the KV cache memory\\nefficiently. This is mainly because they store the KV cache of\\na request in contiguous memory space, as most deep learning\\nframeworks [33, 39] require tensors to be stored in contigu-\\nous memory. However, unlike the tensors in the traditional\\ndeep learning workloads, the KV cache has unique charac-\\nteristics: it dynamically grows and shrinks over time as the\\nmodel generates new tokens, and its lifetime and length are\\nnot known a priori. These characteristics make the existing\\nsystemsâ€™ approach significantly inefficient in two ways:\\nFirst, the existing systems [31, 60] suffer from internal and\\nexternal memory fragmentation. To store the KV cache of\\na request in contiguous space, they pre-allocate a contigu-\\nous chunk of memory with the requestâ€™s maximum length\\n(e.g., 2048 tokens). This can result in severe internal frag-\\nmentation, since the requestâ€™s actual length can be much\\nshorter than its maximum length (e.g., Fig. 11). Moreover,\\neven if the actual length is known a priori, the pre-allocation\\nis still inefficient: As the entire chunk is reserved during the\\nrequestâ€™s lifetime, other shorter requests cannot utilize any\\npart of the chunk that is currently unused. Besides, external\\nmemory fragmentation can also be significant, since the pre-\\nallocated size can be different for each request. Indeed, our\\nprofiling results in Fig. 2 show that only 20.4% - 38.2% of the\\nKV cache memory is used to store the actual token states in\\nthe existing systems.\\nSecond, the existing systems cannot exploit the opportu-\\nnities for memory sharing. LLM services often use advanced\\ndecoding algorithms, such as parallel sampling and beam\\nsearch, that generate multiple outputs per request. In these\\nscenarios, the request consists of multiple sequences that can\\npartially share their KV cache. However, memory sharing is\\nnot possible in the existing systems because the KV cache of\\nthe sequences is stored in separate contiguous spaces.\\nTo address the above limitations, we propose PagedAt-\\ntention, an attention algorithm inspired by the operating\\nsystemâ€™s (OS) solution to memory fragmentation and shar-\\ning: virtual memory with paging . PagedAttention divides the\\nrequestâ€™s KV cache into blocks, each of which can contain\\nthe attention keys and values of a fixed number of tokens. In\\nPagedAttention, the blocks for the KV cache are not neces-\\nsarily stored in contiguous space. Therefore, we can manage\\nthe KV cache in a more flexible way as in OSâ€™s virtual mem-\\nory: one can think of blocks as pages, tokens as bytes, and\\nrequests as processes. This design alleviates internal frag-\\nmentation by using relatively small blocks and allocating\\nthem on demand. Moreover, it eliminates external fragmen-\\ntation as all blocks have the same size. Finally, it enables\\nmemory sharing at the granularity of a block, across the\\ndifferent sequences associated with the same request or even\\nacross the different requests.\\nIn this work, we buildvLLM, a high-throughput distributed\\nLLM serving engine on top of PagedAttention that achieves\\nnear-zero waste in KV cache memory. vLLM uses block-level\\nmemory management and preemptive request scheduling\\nthat are co-designed with PagedAttention. vLLM supports\\npopular LLMs such as GPT [5], OPT [62], and LLaMA [52]\\nwith varying sizes, including the ones exceeding the memory\\ncapacity of a single GPU. Our evaluations on various models\\nand workloads show that vLLM improves the LLM serving\\nthroughput by 2-4 Ã—compared to the state-of-the-art sys-\\ntems [31, 60], without affecting the model accuracy at all. The\\nimprovements are more pronounced with longer sequences,\\nlarger models, and more complex decoding algorithms (Â§4.3).\\nIn summary, we make the following contributions:\\nâ€¢We identify the challenges in memory allocation in serving\\nLLMs and quantify their impact on serving performance.\\nâ€¢We propose PagedAttention, an attention algorithm that\\noperates on KV cache stored in non-contiguous paged\\nmemory, which is inspired by the virtual memory and\\npaging in OS.\\nâ€¢We design and implement vLLM, a distributed LLM serving\\nengine built on top of PagedAttention.\\nâ€¢We evaluate vLLM on various scenarios and demonstrate\\nthat it substantially outperforms the previous state-of-the-\\nart solutions such as FasterTransformer [31] and Orca [60].\\n2 Background\\nIn this section, we describe the generation and serving pro-\\ncedures of typical LLMs and the iteration-level scheduling\\nused in LLM serving.\\n2', metadata={'source': 'vllm.pdf', 'page': 1}),\n",
       " Document(page_content='2.1 Transformer-Based Large Language Models\\nThe task of language modeling is to model the probability\\nof a list of tokens (ğ‘¥1,...,ğ‘¥ ğ‘›).Since language has a natural\\nsequential ordering, it is common to factorize the joint prob-\\nability over the whole sequence as the product of conditional\\nprobabilities (a.k.a. autoregressive decomposition [3]):\\nğ‘ƒ(ğ‘¥)= ğ‘ƒ(ğ‘¥1)Â·ğ‘ƒ(ğ‘¥2 |ğ‘¥1)Â·Â·Â· ğ‘ƒ(ğ‘¥ğ‘› |ğ‘¥1,...,ğ‘¥ ğ‘›âˆ’1). (1)\\nTransformers [53] have become the de facto standard ar-\\nchitecture for modeling the probability above at a large scale.\\nThe most important component of a Transformer-based lan-\\nguage model is its self-attention layers. For an input hidden\\nstate sequence (ğ‘¥1,...,ğ‘¥ ğ‘›) âˆˆRğ‘›Ã—ğ‘‘, a self-attention layer\\nfirst applies linear transformations on each position ğ‘–to get\\nthe query, key, and value vectors:\\nğ‘ğ‘– = ğ‘Šğ‘ğ‘¥ğ‘–, ğ‘˜ğ‘– = ğ‘Šğ‘˜ğ‘¥ğ‘–, ğ‘£ğ‘– = ğ‘Šğ‘£ğ‘¥ğ‘–. (2)\\nThen, the self-attention layer computes the attention score\\nğ‘ğ‘–ğ‘— by multiplying the query vector at one position with all\\nthe key vectors before it and compute the output ğ‘œğ‘– as the\\nweighted average over the value vectors:\\nğ‘ğ‘–ğ‘— = exp(ğ‘âŠ¤\\nğ‘– ğ‘˜ğ‘—/\\nâˆš\\nğ‘‘)\\nÃğ‘–\\nğ‘¡=1 exp(ğ‘âŠ¤\\nğ‘– ğ‘˜ğ‘¡/\\nâˆš\\nğ‘‘)\\n, ğ‘œğ‘– =\\nğ‘–âˆ‘ï¸\\nğ‘—=1\\nğ‘ğ‘–ğ‘—ğ‘£ğ‘—. (3)\\nBesides the computation in Eq. 4, all other components\\nin the Transformer model, including the embedding layer,\\nfeed-forward layer, layer normalization [2], residual connec-\\ntion [22], output logit computation, and the query, key, and\\nvalue transformation in Eq. 2, are all applied independently\\nposition-wise in a form of ğ‘¦ğ‘– = ğ‘“(ğ‘¥ğ‘–).\\n2.2 LLM Service & Autoregressive Generation\\nOnce trained, LLMs are often deployed as a conditional gen-\\neration service (e.g., completion API [34] or chatbot [19, 35]).\\nA request to an LLM service provides a list of input prompt\\ntokens (ğ‘¥1,...,ğ‘¥ ğ‘›),and the LLM service generates a list of\\noutput tokens (ğ‘¥ğ‘›+1,...,ğ‘¥ ğ‘›+ğ‘‡)according to Eq. 1. We refer to\\nthe concatenation of the prompt and output lists as sequence.\\nDue to the decomposition in Eq. 1, the LLM can only sam-\\nple and generate new tokens one by one, and the generation\\nprocess of each new token depends on all theprevious tokens\\nin that sequence, specifically their key and value vectors. In\\nthis sequential generation process, the key and value vectors\\nof existing tokens are often cached for generating future\\ntokens, known as KV cache . Note that the KV cache of one\\ntoken depends on all its previous tokens. This means that the\\nKV cache of the same token appearing at different positions\\nin a sequence will be different.\\nGiven a request prompt, the generation computation in\\nthe LLM service can be decomposed into two phases:\\nThe prompt phase takes the whole user prompt(ğ‘¥1,...,ğ‘¥ ğ‘›)\\nas input and computes the probability of the first new to-\\nken ğ‘ƒ(ğ‘¥ğ‘›+1 |ğ‘¥1,...,ğ‘¥ ğ‘›). During this process, also gener-\\nates the key vectors ğ‘˜1,...,ğ‘˜ ğ‘› and value vectors ğ‘£1,...,ğ‘£ ğ‘›.\\nSince prompt tokens ğ‘¥1,...,ğ‘¥ ğ‘› are all known, the computa-\\ntion of the prompt phase can be parallelized using matrix-\\nmatrix multiplication operations. Therefore, this phase can\\nefficiently use the parallelism inherent in GPUs.\\nThe autoregressive generation phase generates the re-\\nmaining new tokens sequentially. At iteration ğ‘¡, the model\\ntakes one token ğ‘¥ğ‘›+ğ‘¡ as input and computes the probability\\nğ‘ƒ(ğ‘¥ğ‘›+ğ‘¡+1 |ğ‘¥1,...,ğ‘¥ ğ‘›+ğ‘¡)with the key vectors ğ‘˜1,...,ğ‘˜ ğ‘›+ğ‘¡ and\\nvalue vectorsğ‘£1,...,ğ‘£ ğ‘›+ğ‘¡. Note that the key and value vectors\\nat positions 1 to ğ‘›+ğ‘¡âˆ’1 are cached at previous iterations,\\nonly the new key and value vector ğ‘˜ğ‘›+ğ‘¡ and ğ‘£ğ‘›+ğ‘¡ are com-\\nputed at this iteration. This phase completes either when the\\nsequence reaches a maximum length (specified by users or\\nlimited by LLMs) or when an end-of-sequence (<eos>) token\\nis emitted. The computation at different iterations cannot\\nbe parallelized due to the data dependency and often uses\\nmatrix-vector multiplication, which is less efficient. As a re-\\nsult, this phase severely underutilizes GPU computation and\\nbecomes memory-bound, being responsible for most portion\\nof the latency of a single request.\\n2.3 Batching Techniques for LLMs\\nThe compute utilization in serving LLMs can be improved\\nby batching multiple requests. Because the requests share\\nthe same model weights, the overhead of moving weights is\\namortized across the requests in a batch, and can be over-\\nwhelmed by the computational overhead when the batch\\nsize is sufficiently large. However, batching the requests\\nto an LLM service is non-trivial for two reasons. First, the\\nrequests may arrive at different times. A naive batching strat-\\negy would either make earlier requests wait for later ones\\nor delay the incoming requests until earlier ones finish, lead-\\ning to significant queueing delays. Second, the requests may\\nhave vastly different input and output lengths (Fig. 11). A\\nstraightforward batching technique would pad the inputs\\nand outputs of the requests to equalize their lengths, wasting\\nGPU computation and memory.\\nTo address this problem, fine-grained batching mecha-\\nnisms, such as cellular batching [16] and iteration-level sched-\\nuling [60], have been proposed. Unlike traditional methods\\nthat work at the request level, these techniques operate at\\nthe iteration level. After each iteration, completed requests\\nare removed from the batch, and new ones are added. There-\\nfore, a new request can be processed after waiting for a\\nsingle iteration, not waiting for the entire batch to complete.\\nMoreover, with special GPU kernels, these techniques elim-\\ninate the need to pad the inputs and outputs. By reducing\\nthe queueing delay and the inefficiencies from padding, the\\nfine-grained batching mechanisms significantly increase the\\nthroughput of LLM serving.\\n3', metadata={'source': 'vllm.pdf', 'page': 2}),\n",
       " Document(page_content='Four score and seven years ago our fathersbroughtforth <eos><resv> â€¦ <resv> You only live once <eos><resv> â€¦ <resv>\\n2038 slots never used \\n(internal fragmentation)\\n2 slots future used\\n(reserved) External fragmentation\\n7 KV cache states for \\nrequest Aâ€™s prompt\\n3 KV cache states for \\nrequest Bâ€™s prompt\\n1 slot future used\\n(reserved)\\n507 slots never used\\n(Internal fragmentation)\\nRequest B\\ncurrent iteration\\nRequest A\\ncurrent iteration\\n1 slot for\\ngenerated token\\nFigure 3. KV cache memory management in existing systems. Three types of memory wastes â€“ reserved, internal fragmentation,\\nand external fragmentation â€“ exist that prevent other requests from fitting into the memory. The token in each memory slot\\nrepresents its KV cache. Note the same tokens can have different KV cache when at different positions.\\n3 Memory Challenges in LLM Serving\\nAlthough fine-grained batching reduces the waste of com-\\nputing and enables requests to be batched in a more flexible\\nway, the number of requests that can be batched together is\\nstill constrained by GPU memory capacity, particularly the\\nspace allocated to store the KV cache. In other words, the\\nserving systemâ€™s throughput is memory-bound. Overcom-\\ning this memory-bound requires addressing the following\\nchallenges in the memory management:\\nLarge KV cache. The KV Cache size grows quickly with the\\nnumber of requests. As an example, for the 13B parameter\\nOPT model [62], the KV cache of a single token demands 800\\nKB of space, calculated as 2 (key and value vectors) Ã—5120\\n(hidden state size) Ã—40 (number of layers) Ã—2 (bytes per\\nFP16). Since OPT can generate sequences up to 2048 tokens,\\nthe memory required to store the KV cache of one request\\ncan be as much as 1.6 GB. Concurrent GPUs have memory\\ncapacities in the tens of GBs. Even if all available memory\\nwas allocated to KV cache, only a few tens of requests could\\nbe accommodated. Moreover, inefficient memory manage-\\nment can further decrease the batch size, as shown in Fig. 2.\\nAdditionally, given the current trends, the GPUâ€™s computa-\\ntion speed grows faster than the memory capacity [17]. For\\nexample, from NVIDIA A100 to H100, The FLOPS increases\\nby more than 2x, but the GPU memory stays at 80GB max-\\nimum. Therefore, we believe the memory will become an\\nincreasingly significant bottleneck.\\nComplex decoding algorithms. LLM services offer a range\\nof decoding algorithms for users to select from, each with\\nvarying implications for memory management complexity.\\nFor example, when users request multiple random samples\\nfrom a single input prompt, a typical use case in program\\nsuggestion [18], the KV cache of the prompt part, which\\naccounts for 12% of the total KV cache memory in our ex-\\nperiment (Â§6.3), can be shared to minimize memory usage.\\nOn the other hand, the KV cache during the autoregressive\\ngeneration phase should remain unshared due to the dif-\\nferent sample results and their dependence on context and\\nposition. The extent of KV cache sharing depends on the\\nspecific decoding algorithm employed. In more sophisticated\\nalgorithms like beam search [ 49], different request beams\\ncan share larger portions (up to 55% memory saving, see\\nÂ§6.3) of their KV cache, and the sharing pattern evolves as\\nthe decoding process advances.\\nScheduling for unknown input & output lengths. The\\nrequests to an LLM service exhibit variability in their input\\nand output lengths. This requires the memory management\\nsystem to accommodate a wide range of prompt lengths. In\\naddition, as the output length of a request grows at decoding,\\nthe memory required for its KV cache also expands and may\\nexhaust available memory for incoming requests or ongoing\\ngeneration for existing prompts. The system needs to make\\nscheduling decisions, such as deleting or swapping out the\\nKV cache of some requests from GPU memory.\\n3.1 Memory Management in Existing Systems\\nSince most operators in current deep learning frameworks\\n[33, 39] require tensors to be stored in contiguous memory,\\nprevious LLM serving systems [ 31, 60] also store the KV\\ncache of one request as a contiguous tensor across the differ-\\nent positions. Due to the unpredictable output lengths from\\nthe LLM, they statically allocate a chunk of memory for a\\nrequest based on the requestâ€™s maximum possible sequence\\nlength, irrespective of the actual input or eventual output\\nlength of the request.\\nFig. 3 illustrates two requests: request A with 2048 max-\\nimum possible sequence length and request B with a max-\\nimum of 512. The chunk pre-allocation scheme in existing\\nsystems has three primary sources of memory wastes: re-\\nserved slots for future tokens, internal fragmentation due to\\nover-provisioning for potential maximum sequence lengths,\\nand external fragmentation from the memory allocator like\\nthe buddy allocator. The external fragmentation will never\\nbe used for generated tokens, which is known before serving\\na request. Internal fragmentation also remains unused, but\\nthis is only realized after a request has finished sampling.\\nThey are both pure memory waste. Although the reserved\\nmemory is eventually used, reserving this space for the en-\\ntire requestâ€™s duration, especially when the reserved space\\nis large, occupies the space that could otherwise be used to\\nprocess other requests. We visualize the average percentage\\nof memory wastes in our experiments in Fig. 2, revealing\\nthat the actual effective memory in previous systems can be\\nas low as 20.4%.\\n4', metadata={'source': 'vllm.pdf', 'page': 3}),\n",
       " Document(page_content='KV Cache Manager\\nScheduler\\nCPU Block \\nAllocator\\nGPU Block \\nAllocator\\nBlock tables\\nWorker 0\\nModel\\nShard 0\\nCache\\nEngine\\nWorker 1\\nModel\\nShard 1\\nCache\\nEngine\\nWorker N - 1\\nModel\\nShard N - 1\\nCache\\nEngine\\nâ€¦\\nFigure 4. vLLM system overview.\\nAlthough compaction [54] has been proposed as a poten-\\ntial solution to fragmentation, performing compaction in a\\nperformance-sensitive LLM serving system is impractical\\ndue to the massive KV cache. Even with compaction, the\\npre-allocated chunk space for each request prevents memory\\nsharing specific to decoding algorithms in existing memory\\nmanagement systems.\\n4 Method\\nIn this work, we develop a new attention algorithm, Page-\\ndAttention, and build an LLM serving engine,vLLM, to tackle\\nthe challenges outlined in Â§3. The architecture of vLLM is\\nshown in Fig. 4. vLLM adopts a centralized scheduler to\\ncoordinate the execution of distributed GPU workers. The\\nKV cache manager effectively manages the KV cache in a\\npaged fashion, enabled by PagedAttention. Specifically, the\\nKV cache manager manages the physical KV cache memory\\non the GPU workers through the instructions sent by the\\ncentralized scheduler.\\nNext, We describe the PagedAttention algorithm in Â§4.1.\\nWith that, we show the design of the KV cache manager in\\nÂ§4.2 and how it facilitates PagedAttention in Â§4.3, respec-\\ntively. Then, we show how this design facilitates effective\\nmemory management for various decoding methods (Â§4.4)\\nand handles the variable length input and output sequences\\n(Â§4.5). Finally, we show how the system design of vLLM\\nworks in a distributed setting (Â§4.6).\\n4.1 PagedAttention\\nTo address the memory challenges in Â§3, we introduce Page-\\ndAttention, an attention algorithm inspired by the classic idea\\nof paging [25] in operating systems. Unlike the traditional\\nattention algorithms, PagedAttention allows storing continu-\\nous keys and values in non-contiguous memory space. Specif-\\nically, PagedAttention partitions the KV cache of each se-\\nquence into KV blocks. Each block contains the key and value\\nvectors for a fixed number of tokens,1 which we denote asKV\\n1In Transformer, each token has a set of key and value vectors across layers\\nand attention heads within a layer. All the key and value vectors can be\\nmanaged together within a single KV block, or the key and value vectors at\\ndifferent heads and layers can each have a separate block and be managed\\nin separate block tables. The two designs have no performance difference\\nand we choose the second one for easy implementation.\\nforthQuery \\nvector\\nyears ago our fathers\\nbrought forth\\nFour score and seven\\nKey and value vectors\\nBlock 1\\nBlock 2\\nBlock 0\\nFigure 5. Illustration of the PagedAttention algorithm,\\nwhere the attention key and values vectors are stored as\\nnon-contiguous blocks in the memory.\\nblock size (ğµ). Denote the key blockğ¾ğ‘— = (ğ‘˜(ğ‘—âˆ’1)ğµ+1,...,ğ‘˜ ğ‘—ğµ)\\nand value blockğ‘‰ğ‘— = (ğ‘£(ğ‘—âˆ’1)ğµ+1,...,ğ‘£ ğ‘—ğµ).The attention com-\\nputation in Eq. 4 can be transformed into the following block-\\nwise computation:\\nğ´ğ‘–ğ‘— = exp(ğ‘âŠ¤\\nğ‘– ğ¾ğ‘—/\\nâˆš\\nğ‘‘)\\nÃâŒˆğ‘–/ğµâŒ‰\\nğ‘¡=1 exp(ğ‘âŠ¤\\nğ‘– ğ¾ğ‘¡1/\\nâˆš\\nğ‘‘)\\n, ğ‘œğ‘– =\\nâŒˆğ‘–/ğµâŒ‰âˆ‘ï¸\\nğ‘—=1\\nğ‘‰ğ‘—ğ´âŠ¤\\nğ‘–ğ‘—, (4)\\nwhere ğ´ğ‘–ğ‘— = (ğ‘ğ‘–,(ğ‘—âˆ’1)ğµ+1,...,ğ‘ ğ‘–,ğ‘—ğµ)is the row vector of atten-\\ntion score on ğ‘—-th KV block.\\nDuring the attention computation, the PagedAttention\\nkernel identifies and fetches different KV blocks separately.\\nWe show an example of PagedAttention in Fig. 5: The key\\nand value vectors are spread across three blocks, and the\\nthree blocks are not contiguous on the physical memory. At\\neach time, the kernel multiplies the query vector ğ‘ğ‘– of the\\nquery token (â€œforthâ€) and the key vectors ğ¾ğ‘— in a block (e.g.,\\nkey vectors of â€œFour score and seven â€ for block 0) to compute\\nthe attention scoreğ´ğ‘–ğ‘—,and later multipliesğ´ğ‘–ğ‘— with the value\\nvectors ğ‘‰ğ‘— in a block to derive the final attention output ğ‘œğ‘–.\\nIn summary, the PagedAttention algorithm allows the\\nKV blocks to be stored in non-contiguous physical memory,\\nwhich enables more flexible paged memory management in\\nvLLM.\\n4.2 KV Cache Manager\\nThe key idea behind vLLMâ€™s memory manager is analogous\\nto the virtual memory [25] in operating systems. OS parti-\\ntions memory into fixed-sizedpages and maps user programsâ€™\\nlogical pages to physical pages. Contiguous logical pages can\\ncorrespond to non-contiguous physical memory pages, al-\\nlowing user programs to access memory as though it were\\ncontiguous. Moreover, physical memory space needs not to\\nbe fully reserved in advance, enabling the OS to dynamically\\nallocate physical pages as needed. vLLM uses the ideas be-\\nhind virtual memory to manage the KV cache in an LLM\\nservice. Enabled by PagedAttention, we organize the KV\\ncache as fixed-size KV blocks, like pages in virtual memory.\\nA requestâ€™s KV cache is represented as a series of logical\\nKV blocks, filled from left to right as new tokens and their KV\\ncache are generated. The last KV blockâ€™s unfilled positions\\nare reserved for future generations. On GPU workers, ablock\\nengine allocates a contiguous chunk of GPU DRAM and\\n5', metadata={'source': 'vllm.pdf', 'page': 4}),\n",
       " Document(page_content='Request\\nA\\nFour score and seven\\nyears ago our fathers\\nbrought\\nPrompt: â€œFour score and seven years ago ourâ€\\nOutputs: â€œfathersâ€ â†’ â€œbroughtâ€ â†’ â€¦\\nBlock 0\\nBlock 1\\nBlock 2\\nBlock 3\\nyears ago our fathers\\nbrought\\nFour score and seven\\nPhysical KV blocks\\n(on GPU DRAM)\\nBlock 0\\nBlock 1\\nBlock 2\\nBlock 3\\nBlock 4\\nBlock 5\\nBlock 6\\nBlock 7\\nBlock 8\\nLogical KV blocks\\nPhysical block \\nnumber # filled\\n7 4\\n1 3 â†’ 4\\n3 1\\nâ€“ â€“\\nBlock Table1 1 1 1\\n1 1 1 2\\n3\\n1\\n1\\n1\\n1 2\\n3 33\\n1\\n1\\n1\\n1\\n3\\n1 1 1 2\\n1 1 1 1\\n3\\nFigure 6. Block table translation in vLLM.\\ndivides it into physical KV blocks (this is also done on CPU\\nRAM for swapping; see Â§4.5). The KV block manager also\\nmaintains block tables â€”the mapping between logical and\\nphysical KV blocks of each request. Each block table entry\\nrecords the corresponding physical blocks of a logical block\\nand the number of filled positions. Separating logical and\\nphysical KV blocks allows vLLM to dynamically grow the\\nKV cache memory without reserving it for all positions in\\nadvance, which eliminates most memory waste in existing\\nsystems, as in Fig. 2.\\n4.3 Decoding with PagedAttention and vLLM\\nNext, we walk through an example, as in Fig. 6, to demon-\\nstrate how vLLM executes PagedAttention and manages the\\nmemory during the decoding process of a single input se-\\nquence: 1â—‹As in OSâ€™s virtual memory, vLLM does not require\\nreserving the memory for the maximum possible generated\\nsequence length initially. Instead, it reserves only the nec-\\nessary KV blocks to accommodate the KV cache generated\\nduring prompt computation. In this case, The prompt has 7\\ntokens, so vLLM maps the first 2 logical KV blocks (0 and\\n1) to 2 physical KV blocks (7 and 1, respectively). In the\\nprefill step, vLLM generates the KV cache of the prompts\\nand the first output token with a conventional self-attention\\nalgorithm (e.g., [13]). vLLM then stores the KV cache of the\\nfirst 4 tokens in logical block 0 and the following 3 tokens\\nin logical block 1. The remaining slot is reserved for the\\nsubsequent autoregressive generation phase. 2â—‹ In the first\\nautoregressive decoding step, vLLM generates the new token\\nwith the PagedAttention algorithm on physical blocks 7 and\\n1. Since one slot remains available in the last logical block,\\nthe newly generated KV cache is stored there, and the block\\ntableâ€™s #filled record is updated. 3â—‹ At the second decoding\\nstep, as the last logical block is full, vLLM stores the newly\\ngenerated KV cache in a new logical block; vLLM allocates a\\nnew physical block (physical block 3) for it and stores this\\nmapping in the block table.\\nGlobally, for each decoding iteration, vLLM first selects\\na set of candidate sequences for batching (more in Â§4.5),\\nand allocates the physical blocks for the newly required\\nlogical blocks. Then, vLLM concatenates all the input tokens\\nof the current iteration (i.e., all tokens for prompt phase\\nFour score and seven\\nyears ago our fathers\\nbrought\\nBlock 0\\nBlock 1\\nBlock 2\\nBlock 3\\nyears ago our fathers\\nof times\\nbrought\\nIt was the best\\nFour score and seven\\nPhysical KV blocks\\nBlock 0\\nBlock 1\\nBlock 2\\nBlock 3\\nBlock 4\\nBlock 5\\nBlock 6\\nBlock 7\\nBlock 8\\nLogical KV blocks\\nIt was the best\\nof times\\nBlock 0\\nBlock 1\\nBlock 2\\nLogical KV blocks\\nRequest\\nA\\nRequest\\nB\\nFigure 7. Storing the KV cache of two requests at the same\\ntime in vLLM.\\nrequests and the latest tokens for generation phase requests)\\nas one sequence and feeds it into the LLM. During LLMâ€™s\\ncomputation, vLLM uses the PagedAttention kernel to access\\nthe previous KV cache stored in the form of logical KV blocks\\nand saves the newly generated KV cache into the physical\\nKV blocks. Storing multiple tokens within a KV block (block\\nsize > 1) enables the PagedAttention kernel to process the\\nKV cache across more positions in parallel, thus increasing\\nthe hardware utilization and reducing latency. However, a\\nlarger block size also increases memory fragmentation. We\\nstudy the effect of block size in Â§7.2.\\nAgain, vLLM dynamically assigns new physical blocks to\\nlogical blocks as more tokens and their KV cache are gener-\\nated. As all the blocks are filled from left to right and a new\\nphysical block is only allocated when all previous blocks\\nare full, vLLM limits all the memory wastes for a request\\nwithin one block, so it can effectively utilize all the memory,\\nas shown in Fig. 2. This allows more requests to fit into mem-\\nory for batchingâ€”hence improving the throughput. Once a\\nrequest finishes its generation, its KV blocks can be freed to\\nstore the KV cache of other requests. In Fig. 7, we show an\\nexample of vLLM managing the memory for two sequences.\\nThe logical blocks of the two sequences are mapped to differ-\\nent physical blocks within the space reserved by the block\\nengine in GPU workers. The neighboring logical blocks of\\nboth sequences do not need to be contiguous in physical GPU\\nmemory and the space of physical blocks can be effectively\\nutilized by both sequences.\\n4.4 Application to Other Decoding Scenarios\\nÂ§4.3 shows how PagedAttention and vLLM handle basic de-\\ncoding algorithms, such as greedy decoding and sampling,\\nthat take one user prompt as input and generate a single out-\\nput sequence. In many successful LLM applications [18, 34],\\nan LLM service must offer more complex decoding scenarios\\nthat exhibit complex accessing patterns and more opportuni-\\nties for memory sharing. We show the general applicability\\nof vLLM on them in this section.\\nParallel sampling. In LLM-based program assistants [6, 18],\\nan LLM generates multiple sampled outputs for a single in-\\nput prompt; users can choose a favorite output from various\\ncandidates. So far we have implicitly assumed that a request\\n6', metadata={'source': 'vllm.pdf', 'page': 5}),\n",
       " Document(page_content='Sample\\nA1\\nFour score and seven\\nyears ago our fathers\\nBlock 0\\nBlock 1\\nyears ago our mothers\\nyears ago our fathers\\nFour score and seven\\nPhysical KV blocks\\nBlock 0\\nBlock 1\\nBlock 2\\nBlock 3\\nBlock 4\\nBlock 5\\nBlock 6\\nBlock 7\\nBlock 8\\nLogical KV blocks\\nFour score and seven\\nyears ago our mothers\\nBlock 0\\nBlock 1\\nLogical KV blocks\\nSample\\nA2\\nCopy-on-write\\nRef count: 2 â†’ 1\\nFigure 8. Parallel sampling example.\\ngenerates a single sequence. In the remainder of this paper,\\nwe assume the more general case in which a request gener-\\nates multiple sequences. In parallel sampling, one request\\nincludes multiple samples sharing the same input prompt,\\nallowing the KV cache of the prompt to be shared as well. Via\\nits PagedAttention and paged memory management, vLLM\\ncan realize this sharing easily and save memory.\\nFig. 8 shows an example of parallel decoding for two out-\\nputs. Since both outputs share the same prompt, we only\\nreserve space for one copy of the promptâ€™s state at the prompt\\nphase; the logical blocks for the prompts of both sequences\\nare mapped to the same physical blocks: the logical block 0\\nand 1 of both sequences are mapped to physical blocks 7 and\\n1, respectively. Since a single physical block can be mapped\\nto multiple logical blocks, we introduce a reference count for\\neach physical block. In this case, the reference counts for\\nphysical blocks 7 and 1 are both 2. At the generation phase,\\nthe two outputs sample different output tokens and need\\nseparate storage for KV cache. vLLM implements a copy-on-\\nwrite mechanism at the block granularity for the physical\\nblocks that need modification by multiple sequences, similar\\nto the copy-on-write technique in OS virtual memory (e.g.,\\nwhen forking a process). Specifically, in Fig. 8, when sample\\nA1 needs to write to its last logical block (logical block 1),\\nvLLM recognizes that the reference count of the correspond-\\ning physical block (physical block 1) is greater than 1; it\\nallocates a new physical block (physical block 3), instructs\\nthe block engine to copy the information from physical block\\n1, and decreases the reference count to 1. Next, when sample\\nA2 writes to physical block 1, the reference count is already\\nreduced to 1; thus A2 directly writes its newly generated KV\\ncache to physical block 1.\\nIn summary, vLLM enables the sharing of most of the\\nspace used to store the promptsâ€™ KV cache across multiple\\noutput samples, with the exception of the final logical block,\\nwhich is managed by a copy-on-write mechanism. By sharing\\nphysical blocks across multiple samples, memory usage can\\nbe greatly reduced, especially for long input prompts .\\nBeam search. In LLM tasks like machine translation [59],\\nthe users expect the top-ğ‘˜most appropriate translations out-\\nput by the LLM. Beam search [49] is widely used to decode\\nthe most probable output sequence from an LLM, as it miti-\\ngates the computational complexity of fully traversing the\\nBlock 10\\nBlock 11\\nBlock 1 Block 3 Block 6\\nBlock 7\\nBlock 5\\nBlock 0\\nBlock 2 Block 4 Block 8\\nBlock 9\\nBlock 12\\nBeam candidate 0\\nBeam candidate 1\\nBeam candidate 2\\nBeam candidate 3\\nFigure 9. Beam search example.\\nsample space. The algorithm relies on the beam width pa-\\nrameter ğ‘˜, which determines the number of top candidates\\nretained at every step. During decoding, beam search ex-\\npands each candidate sequence in the beam by considering\\nall possible tokens, computes their respective probabilities us-\\ning the LLM, and retains the top-ğ‘˜ most probable sequences\\nout of ğ‘˜Â·|ğ‘‰|candidates, where |ğ‘‰|is the vocabulary size.\\nUnlike parallel decoding, beam search facilities sharing\\nnot only the initial prompt blocks but also other blocks across\\ndifferent candidates, and the sharing patterns dynamically\\nchange as the decoding process advances, similar to the pro-\\ncess tree in the OS created by compound forks. Fig. 9 shows\\nhow vLLM manages the KV blocks for a beam search ex-\\nample with ğ‘˜ = 4. Prior to the iteration illustrated as the\\ndotted line, each candidate sequence has used 4 full logi-\\ncal blocks. All beam candidates share the first block 0 (i.e.,\\nprompt). Candidate 3 digresses from others from the second\\nblock. Candidates 0-2 share the first 3 blocks and diverge at\\nthe fourth block. At subsequent iterations, the top-4 prob-\\nable candidates all originate from candidates 1 and 2. As\\nthe original candidates 0 and 3 are no longer among the\\ntop candidates, their logical blocks are freed, and the refer-\\nence counts of corresponding physical blocks are reduced.\\nvLLM frees all physical blocks whose reference counts reach\\n0 (blocks 2, 4, 5, 8). Then, vLLM allocates new physical blocks\\n(blocks 9-12) to store the new KV cache from the new can-\\ndidates. Now, all candidates share blocks 0, 1, 3; candidates\\n0 and 1 share block 6, and candidates 2 and 3 further share\\nblock 7.\\nPrevious LLM serving systems require frequent memory\\ncopies of the KV cache across the beam candidates. For exam-\\nple, in the case shown in Fig. 9, after the dotted line, candidate\\n3 would need to copy a large portion of candidate 2â€™s KV\\ncache to continue generation. This frequent memory copy\\noverhead is significantly reduced by vLLMâ€™s physical block\\nsharing. In vLLM, most blocks of different beam candidates\\ncan be shared. The copy-on-write mechanism is applied only\\nwhen the newly generated tokens are within an old shared\\nblock, as in parallel decoding. This involves only copying\\none block of data.\\nShared prefix. Commonly, the LLM user provides a (long)\\ndescription of the task including instructions and example\\ninputs and outputs, also known as system prompt [36]. The\\ndescription is concatenated with the actual task input to form\\nthe prompt of the request. The LLM generates outputs based\\n7', metadata={'source': 'vllm.pdf', 'page': 6}),\n",
       " Document(page_content='Translate English to French:\\nâ€œsea otterâ€ => â€œloutre de merâ€\\nâ€œpeppermintâ€ => â€œmenthe poivrÃ©eâ€\\nâ€œplush girafeâ€ => â€œgirafe en pelucheâ€\\nâ€œcheeseâ€ =>\\nâ€œfromageâ€\\nTranslate English to French:\\nâ€œsea otterâ€ => â€œloutre de merâ€\\nâ€œpeppermintâ€ => â€œmenthe poivrÃ©eâ€\\nâ€œplush girafeâ€ => â€œgirafe en pelucheâ€\\nâ€œI love youâ€ =>\\nâ€œJe tâ€™amieâ€\\nShared prefix\\nTask input\\nTask output\\nSequence A\\nPrompt\\nSequence B\\nPrompt\\nSequence A\\nLLM output\\nSequence B\\nLLM output\\nFigure 10. Shared prompt example for machine translation.\\nThe examples are adopted from [5].\\non the full prompt. Fig. 10 shows an example. Moreover, the\\nshared prefix can be further tuned, via prompt engineering,\\nto improve the accuracy of the downstream tasks [26, 27].\\nFor this type of application, many user prompts share a\\nprefix, thus the LLM service provider can store the KV cache\\nof the prefix in advance to reduce the redundant computa-\\ntion spent on the prefix. In vLLM, this can be conveniently\\nachieved by reserving a set of physical blocks for a set of\\npredefined shared prefixes by the LLM service provider, as\\nhow OS handles shared library across processes. A user in-\\nput prompt with the shared prefix can simply map its logi-\\ncal blocks to the cached physical blocks (with the last block\\nmarked copy-on-write). The prompt phase computation only\\nneeds to execute on the userâ€™s task input.\\nMixed decoding methods. The decoding methods dis-\\ncussed earlier exhibit diverse memory sharing and access-\\ning patterns. Nonetheless, vLLM facilitates the simultane-\\nous processing of requests with different decoding prefer-\\nences, which existing systems cannot efficiently do. This is\\nbecause vLLM conceals the complex memory sharing be-\\ntween different sequences via a common mapping layer that\\ntranslates logical blocks to physical blocks. The LLM and\\nits execution kernel only see a list of physical block IDs\\nfor each sequence and do not need to handle sharing pat-\\nterns across sequences. Compared to existing systems, this\\napproach broadens the batching opportunities for requests\\nwith different sampling requirements, ultimately increasing\\nthe systemâ€™s overall throughput.\\n4.5 Scheduling and Preemption\\nWhen the request traffic surpasses the systemâ€™s capacity,\\nvLLM must prioritize a subset of requests. In vLLM, we adopt\\nthe first-come-first-serve (FCFS) scheduling policy for all\\nrequests, ensuring fairness and preventing starvation. When\\nvLLM needs to preempt requests, it ensures that the earliest\\narrived requests are served first and the latest requests are\\npreempted first.\\nLLM services face a unique challenge: the input prompts\\nfor an LLM can vary significantly in length, and the resulting\\noutput lengths are not known a priori, contingent on both\\nthe input prompt and the model. As the number of requests\\nand their outputs grow, vLLM can run out of the GPUâ€™s phys-\\nical blocks to store the newly generated KV cache. There\\nare two classic questions that vLLM needs to answer in this\\ncontext: (1) Which blocks should it evict? (2) How to recover\\nevicted blocks if needed again? Typically, eviction policies\\nuse heuristics to predict which block will be accessed fur-\\nthest in the future and evict that block. Since in our case we\\nknow that all blocks of a sequence are accessed together, we\\nimplement an all-or-nothing eviction policy, i.e., either evict\\nall or none of the blocks of a sequence. Furthermore, multi-\\nple sequences within one request (e.g., beam candidates in\\none beam search request) are gang-scheduled as a sequence\\ngroup. The sequences within one sequence group are always\\npreempted or rescheduled together due to potential memory\\nsharing across those sequences. To answer the second ques-\\ntion of how to recover an evicted block, we consider two\\ntechniques:\\nSwapping. This is the classic technique used by most virtual\\nmemory implementations which copy the evicted pages to a\\nswap space on the disk. In our case, we copy evicted blocks to\\nthe CPU memory. As shown in Fig. 4, besides the GPU block\\nallocator, vLLM includes a CPU block allocator to manage\\nthe physical blocks swapped to CPU RAM. When vLLM\\nexhausts free physical blocks for new tokens, it selects a set\\nof sequences to evict and transfer their KV cache to the CPU.\\nOnce it preempts a sequence and evicts its blocks, vLLM\\nstops accepting new requests until all preempted sequences\\nare completed. Once a request completes, its blocks are freed\\nfrom memory, and the blocks of a preempted sequence are\\nbrought back in to continue the processing of that sequence.\\nNote that with this design, the number of blocks swapped to\\nthe CPU RAM never exceeds the number of total physical\\nblocks in the GPU RAM, so the swap space on the CPU RAM\\nis bounded by the GPU memory allocated for the KV cache.\\nRecomputation. In this case, we simply recompute the KV\\ncache when the preempted sequences are rescheduled. Note\\nthat recomputation latency can be significantly lower than\\nthe original latency, as the tokens generated at decoding\\ncan be concatenated with the original user prompt as a new\\npromptâ€”their KV cache at all positions can be generated in\\none prompt phase iteration.\\nThe performances of swapping and recomputation depend\\non the bandwidth between CPU RAM and GPU memory and\\nthe computation power of the GPU. We examine the speeds\\nof swapping and recomputation in Â§7.3.\\n4.6 Distributed Execution\\nMany LLMs have parameter sizes exceeding the capacity of a\\nsingle GPU [5, 9]. Therefore, it is necessary to partition them\\nacross distributed GPUs and execute them in a model parallel\\nfashion [28, 63]. This calls for a memory manager capable of\\nhandling distributed memory. vLLM is effective in distributed\\nsettings by supporting the widely used Megatron-LM style\\ntensor model parallelism strategy on Transformers [47]. This\\nstrategy adheres to an SPMD (Single Program Multiple Data)\\nexecution schedule, wherein the linear layers are partitioned\\n8', metadata={'source': 'vllm.pdf', 'page': 7}),\n",
       " Document(page_content='Table 1. Model sizes and server configurations.\\nModel size 13B 66B 175B\\nGPUs A100 4 Ã—A100 8 Ã—A100-80GB\\nTotal GPU memory 40 GB 160 GB 640 GB\\nParameter size 26 GB 132 GB 346 GB\\nMemory for KV cache 12 GB 21 GB 264 GB\\nMax. # KV cache slots 15.7K 9.7K 60.1K\\nto perform block-wise matrix multiplication, and the the\\nGPUs constantly synchronize intermediate results via an all-\\nreduce operation. Specifically, the attention operator is split\\non the attention head dimension, each SPMD process takes\\ncare of a subset of attention heads in multi-head attention.\\nWe observe that even with model parallel execution, each\\nmodel shard still processes the same set of input tokens, thus\\nrequiring the KV Cache for the same positions. Therefore,\\nvLLM features a single KV cache manager within the cen-\\ntralized scheduler, as in Fig. 4. Different GPU workers share\\nthe manager, as well as the mapping from logical blocks to\\nphysical blocks. This common mapping allows GPU workers\\nto execute the model with the physical blocks provided by\\nthe scheduler for each input request. Although each GPU\\nworker has the same physical block IDs, a worker only stores\\na portion of the KV cache for its corresponding attention\\nheads.\\nIn each step, the scheduler first prepares the message with\\ninput token IDs for each request in the batch, as well as the\\nblock table for each request. Next, the scheduler broadcasts\\nthis control message to the GPU workers. Then, the GPU\\nworkers start to execute the model with the input token IDs.\\nIn the attention layers, the GPU workers read the KV cache\\naccording to the block table in the control message. During\\nexecution, the GPU workers synchronize the intermediate\\nresults with the all-reduce communication primitive without\\nthe coordination of the scheduler, as in [47]. In the end, the\\nGPU workers send the sampled tokens of this iteration back\\nto the scheduler. In summary, GPU workers do not need\\nto synchronize on memory management as they only need\\nto receive all the memory management information at the\\nbeginning of each decoding iteration along with the step\\ninputs.\\n5 Implementation\\nvLLM is an end-to-end serving system with a FastAPI [15]\\nfrontend and a GPU-based inference engine. The frontend\\nextends the OpenAI API [ 34] interface, allowing users to\\ncustomize sampling parameters for each request, such as\\nthe maximum sequence length and the beam width ğ‘˜. The\\nvLLM engine is written in 8.5K lines of Python and 2K lines of\\nC++/CUDA code. We develop control-related components in-\\ncluding the scheduler and the block manager in Python while\\ndeveloping custom CUDA kernels for key operations such as\\nPagedAttention. For the model executor, we implement pop-\\nular LLMs such as GPT [5], OPT [62], and LLaMA [52] using\\n0 500 1000 1500 2000\\n# T okens\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0Density\\n1eâˆ’2\\nInput (mean: 161.31)\\nOutput (mean: 337.99)\\n(a) ShareGPT\\n0 500 1000 1500 2000\\n# T okens\\n0\\n2\\n4\\n6\\n8Density\\n1eâˆ’2\\nInput (mean: 19.31)\\nOutput (mean: 58.45) (b) Alpaca\\nFigure 11. Input and output length distributions of the (a)\\nShareGPT and (b) Alpaca datasets.\\nPyTorch [39] and Transformers [58]. We use NCCL [32] for\\ntensor communication across the distributed GPU workers.\\n5.1 Kernel-level Optimization\\nSince PagedAttention introduces memory access patterns\\nthat are not efficiently supported by existing systems, we\\ndevelop several GPU kernels for optimizing it. (1) Fused re-\\nshape and block write. In every Transformer layer, the new\\nKV cache are split into blocks, reshaped to a memory layout\\noptimized for block read, then saved at positions specified\\nby the block table. To minimize kernel launch overheads, we\\nfuse them into a single kernel. (2)Fusing block read and atten-\\ntion. We adapt the attention kernel in FasterTransformer [31]\\nto read KV cache according to the block table and perform\\nattention operations on the fly. To ensure coalesced memory\\naccess, we assign a GPU warp to read each block. More-\\nover, we add support for variable sequence lengths within a\\nrequest batch. (3) Fused block copy. Block copy operations,\\nissued by the copy-on-write mechanism, may operate on\\ndiscontinuous blocks. This can lead to numerous invocations\\nof small data movements if we use the cudaMemcpyAsync\\nAPI. To mitigate the overhead, we implement a kernel that\\nbatches the copy operations for different blocks into a single\\nkernel launch.\\n5.2 Supporting Various Decoding Algorithms\\nvLLM implements various decoding algorithms using three\\nkey methods: fork, append, and free. The fork method\\ncreates a new sequence from an existing one. The append\\nmethod appends a new token to the sequence. Finally, the\\nfree method deletes the sequence. For instance, in paral-\\nlel sampling, vLLM creates multiple output sequences from\\nthe single input sequence using the fork method. It then\\nadds new tokens to these sequences in every iteration with\\nappend, and deletes sequences that meet a stopping condi-\\ntion using free. The same strategy is also applied in beam\\nsearch and prefix sharing by vLLM. We believe future decod-\\ning algorithms can also be supported by combining these\\nmethods.\\n6 Evaluation\\nIn this section, we evaluate the performance of vLLM under\\na variety of workloads.\\n9', metadata={'source': 'vllm.pdf', 'page': 8}),\n",
       " Document(page_content='0.0 0.5 1.0 1.5 2.0\\nRequest rate (req/s)\\n(a) OPT-13B, 1 GPU, ShareGPT\\n0.0\\n0.5\\n1.0\\n0.0 0.2 0.4 0.6 0.8 1.0\\nRequest rate (req/s)\\n(b) OPT-66B, 4 GPUs, ShareGPT\\n0.0\\n0.5\\n1.0\\n0.0 0.5 1.0 1.5 2.0 2.5\\nRequest rate (req/s)\\n(c) OPT-175B, 8 GPUs, ShareGPT\\n0.0\\n0.5\\n1.0\\nNormalized latency\\n       (s/token)\\nFasterTransformer Orca (Max) Orca (Pow2) Orca (Oracle) vLLM\\n0 10 20 30\\nRequest rate (req/s)\\n(d) OPT-13B, 1 GPU, Alpaca\\n0.0\\n0.5\\n1.0\\n0 5 10 15 20\\nRequest rate (req/s)\\n(e) OPT-66B, 4 GPUs, Alpaca\\n0.0\\n0.5\\n1.0\\n0 5 10 15 20\\nRequest rate (req/s)\\n(f) OPT-175B, 8 GPUs, Alpaca\\n0.0\\n0.5\\n1.0\\nNormalized latency\\n       (s/token)\\nFigure 12. Single sequence generation with OPT models on the ShareGPT and Alpaca dataset\\nOrca\\n(Max)\\nOrca\\n(Pow2)\\nOrca\\n(Oracle)\\nvLLM\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n35# Batched requests\\n7.00\\n9.81\\n13.62\\n30.42\\n(a) ShareGPT\\nOrca\\n(Max)\\nOrca\\n(Pow2)\\nOrca\\n(Oracle)\\nvLLM\\n0\\n25\\n50\\n75\\n100\\n125\\n150# Batched requests\\n7.00\\n43.24\\n72.75\\n132.44 (b) Alpaca\\nFigure 13. Average number of batched requests when serv-\\ning OPT-13B for the ShareGPT (2 reqs/s) and Alpaca (30\\nreqs/s) traces.\\n6.1 Experimental Setup\\nModel and server configurations. We use OPT [62] mod-\\nels with 13B, 66B, and 175B parameters and LLaMA [52] with\\n13B parameters for our evaluation. 13B and 66B are popular\\nsizes for LLMs as shown in an LLM leaderboard [38], while\\n175B is the size of the famous GPT-3 [ 5] model. For all of\\nour experiments, we use A2 instances with NVIDIA A100\\nGPUs on Google Cloud Platform. The detailed model sizes\\nand server configurations are shown in Table 1.\\nWorkloads.We synthesize workloads based on ShareGPT [51]\\nand Alpaca [50] datasets, which contain input and output\\ntexts of real LLM services. The ShareGPT dataset is a collec-\\ntion of user-shared conversations with ChatGPT [35]. The\\nAlpaca dataset is an instruction dataset generated by GPT-\\n3.5 with self-instruct [57]. We tokenize the datasets and use\\ntheir input and output lengths to synthesize client requests.\\nAs shown in Fig. 11, the ShareGPT dataset has 8.4Ã—longer\\ninput prompts and 5.8Ã—longer outputs on average than the\\nAlpaca dataset, with higher variance. Since these datasets do\\nnot include timestamps, we generate request arrival times\\nusing Poisson distribution with different request rates.\\nBaseline 1: FasterTransformer. FasterTransformer [31] is\\na distributed inference engine highly optimized for latency.\\nAs FasterTransformer does not have its own scheduler, we\\nimplement a custom scheduler with a dynamic batching\\nmechanism similar to the existing serving systems such as\\nTriton [30]. Specifically, we set a maximum batch size ğµas\\nlarge as possible for each experiment, according to the GPU\\nmemory capacity. The scheduler takes up to ğµ number of\\nearliest arrived requests and sends the batch to FasterTrans-\\nformer for processing.\\nBaseline 2: Orca. Orca [60] is a state-of-the-art LLM serving\\nsystem optimized for throughput. Since Orca is not publicly\\navailable for use, we implement our own version of Orca. We\\nassume Orca uses the buddy allocation algorithm to deter-\\nmine the memory address to store KV cache. We implement\\nthree versions of Orca based on how much it over-reserves\\nthe space for request outputs:\\nâ€¢Orca (Oracle). We assume the system has the knowledge\\nof the lengths of the outputs that will be actually generated\\nfor the requests. This shows the upper-bound performance\\nof Orca, which is infeasible to achieve in practice.\\nâ€¢Orca (Pow2). We assume the system over-reserves the\\nspace for outputs by at most 2Ã—. For example, if the true\\noutput length is 25, it reserves 32 positions for outputs.\\nâ€¢Orca (Max). We assume the system always reserves the\\nspace up to the maximum sequence length of the model,\\ni.e., 2048 tokens.\\nKey metrics. We focus on serving throughput. Specifically,\\nusing the workloads with different request rates, we mea-\\nsure normalized latency of the systems, the mean of every\\nrequestâ€™s end-to-end latency divided by its output length,\\nas in Orca [60]. A high-throughput serving system should\\nretain low normalized latency against high request rates.\\nFor most experiments, we evaluate the systems with 1-hour\\ntraces. As an exception, we use 15-minute traces for the\\nOPT-175B model due to the cost limit.\\n10', metadata={'source': 'vllm.pdf', 'page': 9}),\n",
       " Document(page_content='0 5 10 15\\nRequest rate (req/s)\\n(a) parallel generation (parallel size = 2)\\n0.0\\n0.5\\n1.0\\n0 2 4 6 8 10\\nRequest rate (req/s)\\n(b) parallel generation (parallel size = 4)\\n0.0\\n0.5\\n1.0\\n0 2 4 6\\nRequest rate (req/s)\\n(c) parallel generation (parallel size = 6)\\n0.0\\n0.5\\n1.0\\nNormalized latency\\n       (s/token)\\nOrca (Max) Orca (Pow2) Orca (Oracle) vLLM\\n0 5 10 15\\nRequest rate (req/s)\\n(d) beam search (beam width = 2)\\n0.0\\n0.5\\n1.0\\n0 2 4 6 8 10\\nRequest rate (req/s)\\n(e) beam search (beam width = 4)\\n0.0\\n0.5\\n1.0\\n0 2 4 6\\nRequest rate (req/s)\\n(f) beam search (beam width = 6)\\n0.0\\n0.5\\n1.0\\nNormalized latency\\n       (s/token)\\nFigure 14. Parallel generation and beam search with OPT-13B on the Alpaca dataset.\\n6.2 Basic Sampling\\nWe evaluate the performance of vLLM with basic sampling\\n(one sample per request) on three models and two datasets.\\nThe first row of Fig. 12 shows the results on the ShareGPT\\ndataset. The curves illustrate that as the request rate in-\\ncreases, the latency initially increases at a gradual pace but\\nthen suddenly explodes. This can be attributed to the fact\\nthat when the request rate surpasses the capacity of the serv-\\ning system, the queue length continues to grow infinitely\\nand so does the latency of the requests.\\nOn the ShareGPT dataset, vLLM can sustain 1.7Ã—â€“2.7Ã—\\nhigher request rates compared to Orca (Oracle) and2.7Ã—â€“8Ã—\\ncompared to Orca (Max), while maintaining similar laten-\\ncies. This is because vLLMâ€™s PagedAttention can efficiently\\nmanage the memory usage and thus enable batching more\\nrequests than Orca. For example, as shown in Fig. 13a, for\\nOPT-13B vLLM processes 2.2Ã—more requests at the same\\ntime than Orca (Oracle) and 4.3Ã—more requests than Orca\\n(Max). Compared to FasterTransformer, vLLM can sustain up\\nto 22Ã—higher request rates, as FasterTransformer does not\\nutilize a fine-grained scheduling mechanism and inefficiently\\nmanages the memory like Orca (Max).\\nThe second row of Fig. 12 and Fig. 13b shows the results\\non the Alpaca dataset, which follows a similar trend to the\\nShareGPT dataset. One exception is Fig. 12 (f), where vLLMâ€™s\\nadvantage over Orca (Oracle) and Orca (Pow2) is less pro-\\nnounced. This is because the model and server configuration\\nfor OPT-175B (Table 1) allows for large GPU memory space\\navailable to store KV cache, while the Alpaca dataset has\\nshort sequences. In this setup, Orca (Oracle) and Orca (Pow2)\\ncan also batch a large number of requests despite the inef-\\nficiencies in their memory management. As a result, the\\nperformance of the systems becomes compute-bound rather\\nthan memory-bound.\\n2 4 6\\n# Output sequences\\n0\\n4\\n8\\n12Memory saving (%)\\n6.09\\n8.53\\n9.79\\n(a) Parallel sampling\\n2 4 6\\nBeam width\\n0\\n20\\n40\\n60Memory saving (%)\\n37.56\\n53.13 55.16 (b) Beam search\\nFigure 15. Average amount of memory saving from sharing\\nKV blocks, when serving OPT-13B for the Alpaca trace.\\n6.3 Parallel Sampling and Beam Search\\nWe evaluate the effectiveness of memory sharing in Page-\\ndAttention with two popular sampling methods: parallel\\nsampling and beam search. In parallel sampling, all paral-\\nlel sequences in a request can share the KV cache for the\\nprompt. As shown in the first row of Fig. 14, with a larger\\nnumber of sequences to sample, vLLM brings more improve-\\nment over the Orca baselines. Similarly, the second row of\\nFig. 14 shows the results for beam search with different beam\\nwidths. Since beam search allows for more sharing, vLLM\\ndemonstrates even greater performance benefits. The im-\\nprovement of vLLM over Orca (Oracle) on OPT-13B and the\\nAlpaca dataset goes from 1.3Ã—in basic sampling to 2.3Ã—in\\nbeam search with a width of 6.\\nFig. 15 plots the amount of memory saving, computed by\\nthe number of blocks we saved by sharing divided by the\\nnumber of total blocks without sharing. We show 6.1% - 9.8%\\nmemory saving on parallel sampling and 37.6% - 55.2% on\\nbeam search. In the same experiments with the ShareGPT\\ndataset, we saw 16.2% - 30.5% memory saving on parallel\\nsampling and 44.3% - 66.3% on beam search.\\n6.4 Shared prefix\\nWe explore the effectiveness of vLLM for the case a prefix\\nis shared among different input prompts, as illustrated in\\n11', metadata={'source': 'vllm.pdf', 'page': 10}),\n",
       " Document(page_content='0 20 40\\nRequest rate (req/s)\\n(a) 1-shot prefix prompt\\n0.0\\n0.5\\n1.0\\n0 20 40\\nRequest rate (req/s)\\n(b) 5-shot prefix prompt\\n0.0\\n0.5\\n1.0\\nNormalized latency\\n       (s/token)\\nOrca (Oracle) vLLM\\nFigure 16. Translation workload where the input prompts\\nshare a common prefix. The prefix includes (a) 1 example\\nwith 80 tokens or (b) 5 examples with 341 tokens.\\n0.0 0.2 0.4 0.6 0.8\\nRequest rate (req/s)\\n0.0\\n0.5\\n1.0\\nNormalized latency\\n       (s/token)\\nOrca (Max) Orca (Pow2) Orca (Oracle) vLLM\\nFigure 17. Performance on chatbot workload.\\nFig. 10. For the model, we use LLaMA-13B [52], which is mul-\\ntilingual. For the workload, we use the WMT16 [4] English-\\nto-German translation dataset and synthesize two prefixes\\nthat include an instruction and a few translation examples.\\nThe first prefix includes a single example (i.e., one-shot)\\nwhile the other prefix includes 5 examples (i.e., few-shot). As\\nshown in Fig. 16 (a), vLLM achieves 1.67Ã—higher through-\\nput than Orca (Oracle) when the one-shot prefix is shared.\\nFurthermore, when more examples are shared (Fig. 16 (b)),\\nvLLM achieves 3.58Ã—higher throughput than Orca (Oracle).\\n6.5 Chatbot\\nA chatbot [8, 19, 35] is one of the most important applications\\nof LLMs. To implement a chatbot, we let the model generate\\na response by concatenating the chatting history and the\\nlast user query into a prompt. We synthesize the chatting\\nhistory and user query using the ShareGPT dataset. Due to\\nthe limited context length of the OPT-13B model, we cut the\\nprompt to the last 1024 tokens and let the model generate\\nat most 1024 tokens. We do not store the KV cache between\\ndifferent conversation rounds as doing this would occupy the\\nspace for other requests between the conversation rounds.\\nFig. 17 shows that vLLM can sustain 2Ã—higher request\\nrates compared to the three Orca baselines. Since the ShareGPT\\ndataset contains many long conversations, the input prompts\\nfor most requests have 1024 tokens. Due to the buddy allo-\\ncation algorithm, the Orca baselines reserve the space for\\n1024 tokens for the request outputs, regardless of how they\\npredict the output lengths. For this reason, the three Orca\\nbaselines behave similarly. In contrast, vLLM can effectively\\n64 128 256\\nContext length\\n0\\n50\\n100\\n150\\n200\\n250Kernel latency (us)\\nvLLM (bs 8)\\nFT (bs 8)\\nvLLM (bs 32)\\nFT (bs 32)\\n(a) Latency of attention kernels.\\n1 2 4 8 16 32 64 128 256\\nBlock size\\n0.0\\n2.5\\n5.0\\n7.5\\n10.0\\n12.5\\n15.0\\n17.5Normalized latency (s/token)\\nShareGPT\\nAlpaca (b) End-to-end latency with dif-\\nferent block sizes.\\nFigure 18. Ablation experiments.\\nhandle the long prompts, as PagedAttention resolves the\\nproblem of memory fragmentation and reservation.\\n7 Ablation Studies\\nIn this section, we study various aspects of vLLM and evalu-\\nate the design choices we make with ablation experiments.\\n7.1 Kernel Microbenchmark\\nThe dynamic block mapping in PagedAttention affects the\\nperformance of the GPU operations involving the stored KV\\ncache, i.e., block read/writes and attention. Compared to the\\nexisting systems, our GPU kernels (Â§5) involve extra over-\\nheads of accessing the block table, executing extra branches,\\nand handling variable sequence lengths. As shown in Fig. 18a,\\nthis leads to 20â€“26% higher attention kernel latency, com-\\npared to the highly-optimized FasterTransformer implemen-\\ntation. We believe the overhead is small as it only affects\\nthe attention operator but not the other operators in the\\nmodel, such as Linear. Despite the overhead, PagedAttention\\nmakes vLLM significantly outperform FasterTransformer in\\nend-to-end performance (Â§6).\\n7.2 Impact of Block Size\\nThe choice of block size can have a substantial impact on the\\nperformance of vLLM. If the block size is too small, vLLM\\nmay not fully utilize the GPUâ€™s parallelism for reading and\\nprocessing KV cache. If the block size is too large, inter-\\nnal fragmentation increases and the probability of sharing\\ndecreases.\\nIn Fig. 18b, we evaluate the performance of vLLM with dif-\\nferent block sizes, using the ShareGPT and Alpaca traces with\\nbasic sampling under fixed request rates. In the ShareGPT\\ntrace, block sizes from 16 to 128 lead to the best performance.\\nIn the Alpaca trace, while the block size 16 and 32 work\\nwell, larger block sizes significantly degrade the performance\\nsince the sequences become shorter than the block sizes. In\\npractice, we find that the block size 16 is large enough to\\nefficiently utilize the GPU and small enough to avoid signifi-\\ncant internal fragmentation in most workloads. Accordingly,\\nvLLM sets its default block size as 16.\\n12', metadata={'source': 'vllm.pdf', 'page': 11}),\n",
       " Document(page_content='1 2 4 8 16 32 64 128 256\\nBlock size\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\n140Time (ms)\\nRecompute\\nSwap in\\nSwap out\\nSwap in + out\\n(a) Microbenchmark\\n1 2 4 8 16 32 64 128 256\\nBlock size\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5Normalized latency (s/token)\\nRecompute\\nSwap (b) End-to-end performance\\nFigure 19. (a) Overhead of recomputation and swapping for\\ndifferent block sizes. (b) Performance when serving OPT-13B\\nwith the ShareGPT traces at the same request rate.\\n7.3 Comparing Recomputation and Swapping\\nvLLM supports both recomputation and swapping as its re-\\ncovery mechanisms. To understand the tradeoffs between\\nthe two methods, we evaluate their end-to-end performance\\nand microbenchmark their overheads, as presented in Fig. 19.\\nOur results reveal that swapping incurs excessive overhead\\nwith small block sizes. This is because small block sizes often\\nresult in numerous small data transfers between CPU and\\nGPU, which limits the effective PCIe bandwidth. In contrast,\\nthe overhead of recomputation remains constant across dif-\\nferent block sizes, as recomputation does not utilize the KV\\nblocks. Thus, recomputation is more efficient when the block\\nsize is small, while swapping is more efficient when the block\\nsize is large, though recomputation overhead is never higher\\nthan 20% of swappingâ€™s latency. For medium block sizes from\\n16 to 64, the two methods exhibit comparable end-to-end\\nperformance.\\n8 Discussion\\nApplying the virtual memory and paging technique to\\nother GPU workloads. The idea of virtual memory and\\npaging is effective for managing the KV cache in LLM serving\\nbecause the workload requires dynamic memory allocation\\n(since the output length is not known a priori) and its perfor-\\nmance is bound by the GPU memory capacity. However, this\\ndoes not generally hold for every GPU workload. For exam-\\nple, in DNN training, the tensor shapes are typically static,\\nand thus memory allocation can be optimized ahead of time.\\nFor another example, in serving DNNs that are not LLMs,\\nan increase in memory efficiency may not result in any per-\\nformance improvement since the performance is primarily\\ncompute-bound. In such scenarios, introducing the vLLMâ€™s\\ntechniques may rather degrade the performance due to the\\nextra overhead of memory indirection and non-contiguous\\nblock memory. However, we would be excited to see vLLMâ€™s\\ntechniques being applied to other workloads with similar\\nproperties to LLM serving.\\nLLM-specific optimizations in applying virtual mem-\\nory and paging. vLLM re-interprets and augments the idea\\nof virtual memory and paging by leveraging the application-\\nspecific semantics. One example is vLLMâ€™s all-or-nothing\\nswap-out policy, which exploits the fact that processing a\\nrequest requires all of its corresponding token states to be\\nstored in GPU memory. Another example is the recomputa-\\ntion method to recover the evicted blocks, which is not feasi-\\nble in OS. Besides, vLLM mitigates the overhead of memory\\nindirection in paging by fusing the GPU kernels for memory\\naccess operations with those for other operations such as\\nattention.\\n9 Related Work\\nGeneral model serving systems. Model serving has been\\nan active area of research in recent years, with numerous\\nsystems proposed to tackle diverse aspects of deep learning\\nmodel deployment. Clipper [11], TensorFlow Serving [33],\\nNexus [45], InferLine [ 10], and Clockwork [ 20] are some\\nearlier general model serving systems. They study batch-\\ning, caching, placement, and scheduling for serving single\\nor multiple models. More recently, DVABatch [ 12] intro-\\nduces multi-entry multi-exit batching. REEF [21] and Shep-\\nherd [61] propose preemption for serving. AlpaServe [ 28]\\nutilizes model parallelism for statistical multiplexing. How-\\never, these general systems fail to take into account the auto-\\nregressive property and token state of LLM inference, result-\\ning in missed opportunities for optimization.\\nSpecialized serving systems for transformers. Due to\\nthe significance of the transformer architecture, numerous\\nspecialized serving systems for it have been developed. These\\nsystems utilize GPU kernel optimizations [1, 29, 31, 56], ad-\\nvanced batching mechanisms [14, 60], model parallelism [1,\\n41, 60], and parameter sharing [ 64] for efficient serving.\\nAmong them, Orca [60] is most relevant to our approach.\\nComparison to Orca. The iteration-level scheduling in\\nOrca [60] and PagedAttention in vLLM are complementary\\ntechniques: While both systems aim to increase the GPU\\nutilization and hence the throughput of LLM serving, Orca\\nachieves it by scheduling and interleaving the requests so\\nthat more requests can be processed in parallel, while vLLM\\nis doing so by increasing memory utilization so that the\\nworking sets of more requests fit into memory. By reducing\\nmemory fragmentation and enabling sharing, vLLM runs\\nmore requests in a batch in parallel and achieves a 2-4 Ã—\\nspeedup compared to Orca. Indeed, the fine-grained sched-\\nuling and interleaving of the requests like in Orca makes\\nmemory management more challenging, making the tech-\\nniques proposed in vLLM even more crucial.\\nMemory optimizations. The widening gap between the\\ncompute capability and memory capacity of accelerators has\\ncaused memory to become a bottleneck for both training\\nand inference. Swapping [23, 42, 55], recomputation [7, 24]\\nand their combination [40] have been utilized to reduce the\\npeak memory of training. Notably, FlexGen [46] studies how\\nto swap weights and token states for LLM inference with\\n13', metadata={'source': 'vllm.pdf', 'page': 12}),\n",
       " Document(page_content='limited GPU memory, but it does not target the online serv-\\ning settings. OLLA [48] optimizes the lifetime and location\\nof tensors to reduce fragmentation, but it does not do fine-\\ngrained block-level management or online serving. FlashAt-\\ntention [13] applies tiling and kernel optimizations to reduce\\nthe peak memory of attention computation and reduce I/O\\ncosts. This paper introduces a new idea of block-level mem-\\nory management in the context of online serving.\\n10 Conclusion\\nThis paper proposes PagedAttention, a new attention algo-\\nrithm that allows attention keys and values to be stored\\nin non-contiguous paged memory, and presents vLLM, a\\nhigh-throughput LLM serving system with efficient mem-\\nory management enabled by PagedAttention. Inspired by\\noperating systems, we demonstrate how established tech-\\nniques, such as virtual memory and copy-on-write, can be\\nadapted to efficiently manage KV cache and handle various\\ndecoding algorithms in LLM serving. Our experiments show\\nthat vLLM achieves 2-4Ã—throughput improvements over the\\nstate-of-the-art systems.\\nAcknowledgement\\nWe would like to thank Xiaoxuan Liu, Zhifeng Chen, Yan-\\nping Huang, anonymous SOSP reviewers, and our shepherd,\\nLidong Zhou, for their insightful feedback. This research is\\npartly supported by gifts from Andreessen Horowitz, Anyscale,\\nAstronomer, Google, IBM, Intel, Lacework, Microsoft, Mo-\\nhamed Bin Zayed University of Artificial Intelligence, Sam-\\nsung SDS, Uber, and VMware.\\nReferences\\n[1] Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, Am-\\nmar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Jeff Rasley, Shaden\\nSmith, Olatunji Ruwase, et al. 2022. DeepSpeed Inference: Enabling\\nEfficient Inference of Transformer Models at Unprecedented Scale.\\narXiv preprint arXiv:2207.00032 (2022).\\n[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer\\nnormalization. arXiv preprint arXiv:1607.06450 (2016).\\n[3] Yoshua Bengio, RÃ©jean Ducharme, and Pascal Vincent. 2000. A neural\\nprobabilistic language model. Advances in neural information process-\\ning systems 13 (2000).\\n[4] Ond rej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Gra-\\nham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp\\nKoehn, Varvara Logacheva, Christof Monz, Matteo Negri, Aurelie\\nNeveol, Mariana Neves, Martin Popel, Matt Post, Raphael Rubino, Car-\\nolina Scarton, Lucia Specia, Marco Turchi, Karin Verspoor, and Marcos\\nZampieri. 2016. Findings of the 2016 Conference on Machine Trans-\\nlation. In Proceedings of the First Conference on Machine Translation .\\nAssociation for Computational Linguistics, Berlin, Germany, 131â€“198.\\nhttp://www.aclweb.org/anthology/W/W16/W16-2301\\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D\\nKaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish\\nSastry, Amanda Askell, et al . 2020. Language models are few-shot\\nlearners. Advances in neural information processing systems 33 (2020),\\n1877â€“1901.\\n[6] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde\\nde Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas\\nJoseph, Greg Brockman, et al. 2021. Evaluating large language models\\ntrained on code. arXiv preprint arXiv:2107.03374 (2021).\\n[7] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016.\\nTraining deep nets with sublinear memory cost. arXiv preprint\\narXiv:1604.06174 (2016).\\n[8] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao\\nZhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E.\\nGonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An Open-Source\\nChatbot Impressing GPT-4 with 90%* ChatGPT Quality. https://lmsys.\\norg/blog/2023-03-30-vicuna/\\n[9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma,\\nGaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung,\\nCharles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling lan-\\nguage modeling with pathways.arXiv preprint arXiv:2204.02311 (2022).\\n[10] Daniel Crankshaw, Gur-Eyal Sela, Xiangxi Mo, Corey Zumar, Ion\\nStoica, Joseph Gonzalez, and Alexey Tumanov. 2020. InferLine: latency-\\naware provisioning and scaling for prediction serving pipelines. In\\nProceedings of the 11th ACM Symposium on Cloud Computing . 477â€“491.\\n[11] Daniel Crankshaw, Xin Wang, Guilio Zhou, Michael J Franklin,\\nJoseph E Gonzalez, and Ion Stoica. 2017. Clipper: A Low-Latency\\nOnline Prediction Serving System. In 14th USENIX Symposium on\\nNetworked Systems Design and Implementation (NSDI 17) . 613â€“627.\\n[12] Weihao Cui, Han Zhao, Quan Chen, Hao Wei, Zirui Li, Deze Zeng,\\nChao Li, and Minyi Guo. 2022. DVABatch: Diversity-aware Multi-\\nEntry Multi-Exit Batching for Efficient Processing of DNN Services\\non GPUs. In 2022 USENIX Annual Technical Conference (USENIX ATC\\n22). 183â€“198.\\n[13] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher RÃ©.\\n2022. Flashattention: Fast and memory-efficient exact attention with\\nio-awareness. Advances in Neural Information Processing Systems 35\\n(2022), 16344â€“16359.\\n[14] Jiarui Fang, Yang Yu, Chengduo Zhao, and Jie Zhou. 2021. TurboTrans-\\nformers: an efficient GPU serving system for transformer models. In\\nProceedings of the 26th ACM SIGPLAN Symposium on Principles and\\nPractice of Parallel Programming . 389â€“402.\\n[15] FastAPI. 2023. FastAPI. https://github.com/tiangolo/fastapi.\\n[16] Pin Gao, Lingfan Yu, Yongwei Wu, and Jinyang Li. 2018. Low latency\\nrnn inference with cellular batching. In Proceedings of the Thirteenth\\nEuroSys Conference. 1â€“15.\\n[17] Amir Gholami, Zhewei Yao, Sehoon Kim, Michael W Mahoney, and\\nKurt Keutzer. 2021. Ai and memory wall.RiseLab Medium Post 1 (2021),\\n6.\\n[18] Github. 2022. https://github.com/features/copilot\\n[19] Google. 2023. https://bard.google.com/\\n[20] Arpan Gujarati, Reza Karimi, Safya Alzayat, Wei Hao, Antoine Kauf-\\nmann, Ymir Vigfusson, and Jonathan Mace. 2020. Serving{DNNs}like\\nClockwork: Performance Predictability from the Bottom Up. In 14th\\nUSENIX Symposium on Operating Systems Design and Implementation\\n(OSDI 20). 443â€“462.\\n[21] Mingcong Han, Hanze Zhang, Rong Chen, and Haibo Chen.\\n2022. Microsecond-scale Preemption for Concurrent {GPU-\\naccelerated}{DNN}Inferences. In 16th USENIX Symposium on Oper-\\nating Systems Design and Implementation (OSDI 22) . 539â€“558.\\n[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep\\nresidual learning for image recognition. In Proceedings of the IEEE\\nconference on computer vision and pattern recognition . 770â€“778.\\n[23] Chien-Chin Huang, Gu Jin, and Jinyang Li. 2020. Swapadvisor: Push-\\ning deep learning beyond the gpu memory limit via smart swapping.\\nIn Proceedings of the Twenty-Fifth International Conference on Archi-\\ntectural Support for Programming Languages and Operating Systems .\\n1341â€“1355.\\n[24] Paras Jain, Ajay Jain, Aniruddha Nrusimha, Amir Gholami, Pieter\\nAbbeel, Joseph Gonzalez, Kurt Keutzer, and Ion Stoica. 2020. Check-\\nmate: Breaking the memory wall with optimal tensor rematerialization.\\n14', metadata={'source': 'vllm.pdf', 'page': 13}),\n",
       " Document(page_content='Proceedings of Machine Learning and Systems 2 (2020), 497â€“511.\\n[25] Tom Kilburn, David BG Edwards, Michael J Lanigan, and Frank H\\nSumner. 1962. One-level storage system. IRE Transactions on Electronic\\nComputers 2 (1962), 223â€“235.\\n[26] Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power\\nof scale for parameter-efficient prompt tuning. arXiv preprint\\narXiv:2104.08691 (2021).\\n[27] Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing contin-\\nuous prompts for generation. arXiv preprint arXiv:2101.00190 (2021).\\n[28] Zhuohan Li, Lianmin Zheng, Yinmin Zhong, Vincent Liu, Ying Sheng,\\nXin Jin, Yanping Huang, Zhifeng Chen, Hao Zhang, Joseph E Gonzalez,\\net al. 2023. AlpaServe: Statistical Multiplexing with Model Parallelism\\nfor Deep Learning Serving. arXiv preprint arXiv:2302.11665 (2023).\\n[29] Lingxiao Ma, Zhiqiang Xie, Zhi Yang, Jilong Xue, Youshan Miao, Wei\\nCui, Wenxiang Hu, Fan Yang, Lintao Zhang, and Lidong Zhou. 2020.\\nRammer: Enabling holistic deep learning compiler optimizations with\\nrtasks. In Proceedings of the 14th USENIX Conference on Operating\\nSystems Design and Implementation . 881â€“897.\\n[30] NVIDIA. [n. d.]. Triton Inference Server. https://developer.nvidia.com/\\nnvidia-triton-inference-server .\\n[31] NVIDIA. 2023. FasterTransformer. https://github.com/NVIDIA/\\nFasterTransformer.\\n[32] NVIDIA. 2023. NCCL: The NVIDIA Collective Communication Library.\\nhttps://developer.nvidia.com/nccl.\\n[33] Christopher Olston, Noah Fiedel, Kiril Gorovoy, Jeremiah Harmsen, Li\\nLao, Fangwei Li, Vinu Rajashekhar, Sukriti Ramesh, and Jordan Soyke.\\n2017. Tensorflow-serving: Flexible, high-performance ml serving.\\narXiv preprint arXiv:1712.06139 (2017).\\n[34] OpenAI. 2020. https://openai.com/blog/openai-api\\n[35] OpenAI. 2022. https://openai.com/blog/chatgpt\\n[36] OpenAI. 2023. https://openai.com/blog/custom-instructions-for-\\nchatgpt\\n[37] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]\\n[38] LMSYS ORG. 2023. Chatbot Arena Leaderboard Week 8: Introduc-\\ning MT-Bench and Vicuna-33B. https://lmsys.org/blog/2023-06-22-\\nleaderboard/.\\n[39] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James\\nBradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia\\nGimelshein, Luca Antiga, et al . 2019. Pytorch: An imperative style,\\nhigh-performance deep learning library. Advances in neural informa-\\ntion processing systems 32 (2019).\\n[40] Shishir G Patil, Paras Jain, Prabal Dutta, Ion Stoica, and Joseph Gon-\\nzalez. 2022. POET: Training Neural Networks on Tiny Devices with\\nIntegrated Rematerialization and Paging. In International Conference\\non Machine Learning . PMLR, 17573â€“17583.\\n[41] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin,\\nJames Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani\\nAgrawal, and Jeff Dean. 2022. Efficiently Scaling Transformer Inference.\\narXiv preprint arXiv:2211.05102 (2022).\\n[42] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji\\nRuwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He.\\n2021. ZeRO-Offload: Democratizing Billion-Scale Model Training.. In\\nUSENIX Annual Technical Conference . 551â€“564.\\n[43] Reuters. 2023. https://www.reuters.com/technology/tech-giants-ai-\\nlike-bing-bard-poses-billion-dollar-search-problem-2023-02-22/\\n[44] Amazon Web Services. 2023. https://aws.amazon.com/bedrock/\\n[45] Haichen Shen, Lequn Chen, Yuchen Jin, Liangyu Zhao, Bingyu Kong,\\nMatthai Philipose, Arvind Krishnamurthy, and Ravi Sundaram. 2019.\\nNexus: A GPU cluster engine for accelerating DNN-based video anal-\\nysis. In Proceedings of the 27th ACM Symposium on Operating Systems\\nPrinciples. 322â€“337.\\n[46] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin,\\nDaniel Y Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E Gon-\\nzalez, et al . 2023. High-throughput Generative Inference of Large\\nLanguage Models with a Single GPU. arXiv preprint arXiv:2303.06865\\n(2023).\\n[47] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley,\\nJared Casper, and Bryan Catanzaro. 2019. Megatron-lm: Training multi-\\nbillion parameter language models using model parallelism. arXiv\\npreprint arXiv:1909.08053 (2019).\\n[48] Benoit Steiner, Mostafa Elhoushi, Jacob Kahn, and James Hegarty. 2022.\\nOLLA: Optimizing the Lifetime and Location of Arrays to Reduce the\\nMemory Usage of Neural Networks. (2022). https://doi.org/10.48550/\\narXiv.2210.12924\\n[49] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to se-\\nquence learning with neural networks. Advances in neural information\\nprocessing systems 27 (2014).\\n[50] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen\\nLi, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023.\\nStanford Alpaca: An Instruction-following LLaMA model. https://\\ngithub.com/tatsu-lab/stanford_alpaca.\\n[51] ShareGPT Team. 2023. https://sharegpt.com/\\n[52] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-\\nAnne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric\\nHambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation\\nlanguage models. arXiv preprint arXiv:2302.13971 (2023).\\n[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\\nJones, Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. At-\\ntention is all you need. Advances in neural information processing\\nsystems 30 (2017).\\n[54] Jing Wang, Youyou Lu, Qing Wang, Minhui Xie, Keji Huang, and Jiwu\\nShu. 2022. Pacman: An Efficient Compaction Approach for {Log-\\nStructured}{Key-Value}Store on Persistent Memory. In 2022 USENIX\\nAnnual Technical Conference (USENIX ATC 22) . 773â€“788.\\n[55] Linnan Wang, Jinmian Ye, Yiyang Zhao, Wei Wu, Ang Li, Shuai-\\nwen Leon Song, Zenglin Xu, and Tim Kraska. 2018. Superneurons: Dy-\\nnamic GPU memory management for training deep neural networks.\\nIn Proceedings of the 23rd ACM SIGPLAN symposium on principles and\\npractice of parallel programming . 41â€“53.\\n[56] Xiaohui Wang, Ying Xiong, Yang Wei, Mingxuan Wang, and Lei Li.\\n2021. LightSeq: A High Performance Inference Library for Transform-\\ners. In Proceedings of the 2021 Conference of the North American Chapter\\nof the Association for Computational Linguistics: Human Language Tech-\\nnologies: Industry Papers . 113â€“120.\\n[57] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A\\nSmith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-Instruct:\\nAligning Language Model with Self Generated Instructions. arXiv\\npreprint arXiv:2212.10560 (2022).\\n[58] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,\\nClement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, RÃ©mi Louf,\\nMorgan Funtowicz, et al. 2020. Transformers: State-of-the-art natural\\nlanguage processing. In Proceedings of the 2020 conference on empirical\\nmethods in natural language processing: system demonstrations . 38â€“45.\\n[59] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad\\nNorouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao,\\nKlaus Macherey, et al . 2016. Googleâ€™s neural machine translation\\nsystem: Bridging the gap between human and machine translation.\\narXiv preprint arXiv:1609.08144 (2016).\\n[60] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and\\nByung-Gon Chun. 2022. Orca: A Distributed Serving System for\\n{Transformer-Based}Generative Models. In 16th USENIX Symposium\\non Operating Systems Design and Implementation (OSDI 22) . 521â€“538.\\n[61] Hong Zhang, Yupeng Tang, Anurag Khandelwal, and Ion Stoica. 2023.\\nSHEPHERD: Serving DNNs in the Wild. In20th USENIX Symposium on\\nNetworked Systems Design and Implementation (NSDI 23) . USENIX As-\\nsociation, Boston, MA, 787â€“808. https://www.usenix.org/conference/\\nnsdi23/presentation/zhang-hong\\n15', metadata={'source': 'vllm.pdf', 'page': 14}),\n",
       " Document(page_content='[62] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen,\\nShuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria\\nLin, et al. 2022. Opt: Open pre-trained transformer language models.\\narXiv preprint arXiv:2205.01068 (2022).\\n[63] Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng\\nChen, Yanping Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo,\\nEric P Xing, et al. 2022. Alpa: Automating Inter-and Intra-Operator\\nParallelism for Distributed Deep Learning. In 16th USENIX Symposium\\non Operating Systems Design and Implementation (OSDI 22) . 559â€“578.\\n[64] Zhe Zhou, Xuechao Wei, Jiejing Zhang, and Guangyu Sun. 2022. PetS:\\nA Unified Framework for Parameter-Efficient Transformers Serving. In\\n2022 USENIX Annual Technical Conference (USENIX ATC 22) . 489â€“504.\\n16', metadata={'source': 'vllm.pdf', 'page': 15})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Data Ingestion\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader(\"vllm.pdf\")\n",
    "docs =loader.load()\n",
    "docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Split into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "#print(documents[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at: ./local_models/all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# Download the embedding model from Hugging Face\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Define model name and local path\n",
    "hf_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "local_path = \"./local_models/all-MiniLM-L6-v2\"\n",
    "\n",
    "# Load and download the model from Hugging Face\n",
    "model = SentenceTransformer(hf_model_name)\n",
    "\n",
    "# Save the entire model to the local directory\n",
    "model.save(local_path)\n",
    "\n",
    "print(f\"Model saved at: {local_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index created and saved.\n"
     ]
    }
   ],
   "source": [
    "# Generate Embeddings and Create a FAISS Index\n",
    "# This step converts each chunk of your PDF into a vector \n",
    "# (using a sentence embedding model), and stores them in a FAISS vector store for fast similarity search.\n",
    "# We'll now use the local model with langchain to embed your document chunks and store them in a FAISS vector store\n",
    "# for fast retrieval.\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Step 1: Load local model for embeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"./local_models/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Step 2: Create FAISS vector store from your document chunks\n",
    "vectorstore = FAISS.from_documents(documents, embedding_model)\n",
    "\n",
    "# Step 3: Optionally save the FAISS index to disk\n",
    "vectorstore.save_local(\"quantization_faiss_index\")\n",
    "\n",
    "print(\"FAISS index created and saved.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect LangChain to the Running Ollama Mistral Model\n",
    "from langchain.llms import Ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Connect to the locally running Mistral model\n",
    "llm = Ollama(model=\"mistral\")  # Default port is http://localhost:11434\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up RAG with FAISS retriever\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  The main benefit of PagedAttention in vLLM is that it allows for more efficient management of memory usage, enabling the processing of a larger number of requests at the same time compared to systems like Orca (Max) and FasterTransformer. This is achieved by storing continuous keys and values in non-contiguous memory space, which allows for batching more requests than these other systems.\n"
     ]
    }
   ],
   "source": [
    "# Ask your question\n",
    "query = \"What is main benefit of PageAttention in vLLM?\"\n",
    "result = qa_chain(query)\n",
    "\n",
    "print(\"Answer:\", result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‘‹ Welcome to the vLLM Q&A Assistant!\n",
      "ğŸ’¡ Ask one question related to vLLM, PagedAttention, or quantization.\n",
      "\n",
      "â“ Question: What is main benefit of PageAttention in vLLM?\n",
      "\n",
      "ğŸ“˜ Answer: The main benefit of PagedAttention in vLLM is its ability to manage memory usage efficiently, allowing it to process more requests at a time compared to other models such as Orca and FasterTransformer. This is made possible by storing continuous keys and values in non-contiguous memory space, which enables batching more requests, resulting in higher request rates.\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ‘‹ Welcome to the vLLM Q&A Assistant!\")\n",
    "print(\"ğŸ’¡ Ask one question related to vLLM, PagedAttention, or quantization.\\n\")\n",
    "\n",
    "# Your predefined question\n",
    "query = \"What is main benefit of PageAttention in vLLM?\"\n",
    "print(f\"â“ Question: {query}\")\n",
    "\n",
    "# Retrieve relevant docs\n",
    "docs = vectorstore.similarity_search(query, k=2)\n",
    "\n",
    "# Fallback if nothing relevant is found\n",
    "if not docs or all(len(doc.page_content.strip()) < 20 for doc in docs):\n",
    "    print(\"ğŸ¤– Hi! I'm here to help with questions about vLLM and related topics.\")\n",
    "else:\n",
    "    # Run the RAG pipeline\n",
    "    result = qa_chain.invoke(query)\n",
    "\n",
    "    # Extract and print only the first sentence of the answer\n",
    "    answer = result[\"result\"].strip().split('\\n')[0]\n",
    "    print(f\"\\nğŸ“˜ Answer: {answer}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
